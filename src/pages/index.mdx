---
layout: ../layouts/Layout.astro
title: "VideoAds: Benchmarking Multi-Modal Language Models for Complex Temporal Understanding"
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import Splat from "../components/Splat.tsx"
import visual_dataset from "../assets/visual_dataset.png";
import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";

export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Zheyuan Zhang",
      institution: "Northwestern University",
    },
    {
      name: "Monica Dou",
      institution: "Northwestern University",
    },
    {
      name: "Linkai Peng",
      institution: "Northwestern University",
    },
    {
      name: "Hongyi Pan",
      institution: "Northwestern University",
    },
    {
      name: "Ulas Bagci",
      institution: "Northwestern University",
    },
    {
      name: "Boqing Gong",
      institution: "Boston University",
    },
  ]}
  conference="ICCV 2025"
  notes={[
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "https://github.com/freshman97/VideoAds",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
    {
      name: "Dataset",
      url: "https://huggingface.co/datasets/Zheyuan14/VideoAds",
      icon: "simple-icons:huggingface"
    }
  ]}
  />

<HighlightedSection>

## Abstract

Advertisement videos serve as a rich and valuable source of purpose-driven information, encompassing high-quality visual, textual, and contextual cues designed to engage viewers. They are often more complex than general videos of similar duration due to their structured narratives and rapid scene transitions, posing significant challenges to multimodal large language models (MLLMs). In this work, we introduce VideoAds, the first dataset tailored for benchmarking the performance of MLLMs on advertisement videos. VideoAds comprises well-curated advertisement videos with complex temporal structures, accompanied by manually annotated diverse questions across three core tasks: visual finding, video summary, and visual reasoning. We propose a quantitative measure to compare VideoAds against existing benchmarks in terms of video complexity. Through extensive experiments, we find that Qwen2.5-VL-72B, an opensource MLLM, achieves 73.35% accuracy on VideoAds, outperforming GPT-4o (66.82%) and Gemini-1.5 Pro (69.66%); the two proprietary models especially fall behind the opensource model in video summarization and reasoning, but perform the best in visual finding. Notably, human experts easily achieve a remarkable accuracy of 94.27%. These results underscore the necessity of advancing MLLMs’ temporal modeling capabilities and highlight VideoAds as a potentially pivotal benchmark for future research in understanding video that requires high FPS sampling. 

</HighlightedSection>

## Figures

VideoAds comprises three challenging tasks: Visual Finding, Visual Summary, and Visual Reasoning, specifically designed to evaluate MLLMs’ temporal reasoning capabilities on videos with complex temporal structures that have never been investigated before. Unlike many previous datasets that focus on recognizing isolated actions or events, VideoAds demands that models derive the correct answers only through multistep reasoning over multi-modal visual clues.


<Figure
    caption="An example from our curated video question answering dataset."
  >
    <Image source={visual_dataset} altText="An example from our curated video question answering dataset." />
</Figure>

## BibTeX citation

```bibtex
@article{zhang2025videoads,
  title={VideoAds for Fast-Paced Video Understanding: Where Opensource Foundation Models Beat GPT-4o \& Gemini-1.5 Pro},
  author={Zhang, Zheyuan and Dou, Monica and Peng, Linkai and Pan, Hongyi and Bagci, Ulas and Gong, Boqing},
  journal={arXiv preprint arXiv:2504.09282},
  year={2025}
}
```